{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd0792b-9402-4849-bd42-fcc528acae2e",
   "metadata": {},
   "source": [
    "# Project: Building a RAG chatbot using python, Langchain and a pre-defined knowledge-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e7b891-f6e0-49ff-9a95-01daa6bad7bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## What is RAG?\n",
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation,\n",
    "\n",
    "**This tutorial will show how to build a simple Q&A application over a text data source**\n",
    "### Overview:\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "### Indexing\n",
    "\n",
    "1. **Load**:\n",
    "   - First, we need to load our data. This is done using **Document Loaders**.\n",
    "   - Document loaders handle various file formats (e.g., text files, PDFs, web pages) and convert them into a standardized format for processing.\n",
    "\n",
    "2. **Split**:\n",
    "   - Text splitters break large documents into smaller chunks.\n",
    "   - This is useful for both indexing data and passing it into a model, as:\n",
    "     - Large chunks are harder to search over.\n",
    "     - They may not fit in a model's finite context window.\n",
    "\n",
    "3. **Store**:\n",
    "   - We need a place to store and index our splits so they can be searched over later.\n",
    "   - This is often done using a **VectorStore** and an **Embeddings model**.\n",
    "   - The embeddings model converts text into numerical vectors, and the vector store allows for efficient similarity search.\n",
    "\n",
    "### Retrieval and generation\n",
    "\n",
    "1. **Retrieve**:\n",
    "   - Given a user input, relevant splits are retrieved from storage using a **Retriever**.\n",
    "\n",
    "2. **Generate**:\n",
    "   - A **ChatModel** / **LLM** produces an answer using a prompt that includes both the question and the retrieved data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ceeb5-484e-487f-a654-ccd62e0a9fcd",
   "metadata": {},
   "source": [
    "# 2. Building the Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570348d-686b-4d58-bc19-347efcd842e3",
   "metadata": {},
   "source": [
    "### 1. Installing required packages\n",
    "\n",
    "for this tutorial we need the following python packages:\n",
    "\n",
    "\n",
    "1. **langchain-text-splitters**: A library for splitting text into chunks (e.g., for RAG pipelines).\n",
    "2. **langchain-community**: A collection of community-contributed tools and integrations for LangChain.\n",
    "3. **langchain[mistralai]**: A wrapper that connects our application with **open-source** and **free** **ChatModel**.\n",
    "4. **langchain-mistralai**: A library for integrating Mistral AI models with LangChain.\n",
    "5. **langchain-core**: Provides core functionalities for LangChain, including an in-memory vector store.\n",
    "6. **faiss-cpu** : faiss-cpu: A library for efficient similarity search and clustering of dense vectors (CPU version).\n",
    "7. **Flask**: A lightweight Python web framework for building APIs and web servers.  \n",
    "8. **Flask-CORS**: Enables Cross-Origin Resource Sharing (CORS) in Flask for frontend-backend communication.  \n",
    "9. **python-dotenv**: Loads environment variables from a `.env` file for secure configuration management.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5aa660-f8ec-47b7-bf13-aaa3db8bd4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in ./.venv/lib/python3.12/site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.12/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in ./.venv/lib/python3.12/site-packages (from langchain-text-splitters) (0.3.47)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in ./.venv/lib/python3.12/site-packages (from langchain-community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain-community) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain-community) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain-community) (0.3.18)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in ./.venv/lib/python3.12/site-packages (from langchain-community) (2.2.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-text-splitters) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain[mistralai] in ./.venv/lib/python3.12/site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (0.3.47)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (0.3.18)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (6.0.2)\n",
      "Requirement already satisfied: langchain-mistralai in ./.venv/lib/python3.12/site-packages (from langchain[mistralai]) (0.2.9)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain[mistralai]) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain[mistralai]) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain[mistralai]) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain[mistralai]) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain[mistralai]) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain[mistralai]) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain[mistralai]) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain[mistralai]) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[mistralai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[mistralai]) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain[mistralai]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain[mistralai]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain[mistralai]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain[mistralai]) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain[mistralai]) (3.1.1)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai->langchain[mistralai]) (0.21.1)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai->langchain[mistralai]) (0.4.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[mistralai]) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[mistralai]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[mistralai]) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain[mistralai]) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai->langchain[mistralai]) (0.29.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai->langchain[mistralai]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai->langchain[mistralai]) (2025.3.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai->langchain[mistralai]) (4.67.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain[mistralai]) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-mistralai in ./.venv/lib/python3.12/site-packages (0.2.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai) (0.3.47)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai) (0.21.1)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai) (0.4.0)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain-mistralai) (2.10.6)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (0.3.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain-mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain-mistralai) (2.27.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.29.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2025.3.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.67.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-mistralai) (0.23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in ./.venv/lib/python3.12/site-packages (0.3.47)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain-core) (0.3.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.12/site-packages (from langchain-core) (2.10.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.27.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.12/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./.venv/lib/python3.12/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in ./.venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in ./.venv/lib/python3.12/site-packages (from Flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./.venv/lib/python3.12/site-packages (from Flask) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in ./.venv/lib/python3.12/site-packages (from Flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in ./.venv/lib/python3.12/site-packages (from Flask) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in ./.venv/lib/python3.12/site-packages (from Flask) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask-cors in ./.venv/lib/python3.12/site-packages (5.0.1)\n",
      "Requirement already satisfied: flask>=0.9 in ./.venv/lib/python3.12/site-packages (from flask-cors) (3.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in ./.venv/lib/python3.12/site-packages (from flask-cors) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./.venv/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in ./.venv/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in ./.venv/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in ./.venv/lib/python3.12/site-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.12/site-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# This command installs or upgrades specific Python packages using pip.\n",
    "# install: The pip subcommand used to install Python packages.\n",
    "\n",
    "# --upgrade or -U: Upgrades the specified packages to their latest versions if they are already installed.\n",
    "\n",
    "!pip install -U langchain-text-splitters langchain-community \n",
    "!pip install -U \"langchain[mistralai]\"\n",
    "!pip install -U langchain-mistralai\n",
    "!pip install -U langchain-core\n",
    "!pip install faiss-cpu\n",
    "!pip install Flask\n",
    "!pip install python-dotenv\n",
    "!pip install flask-cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e8148-f600-4dd8-ac80-e1c443a88a79",
   "metadata": {},
   "source": [
    "## 2. Detailed walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915aecab-415e-4a4e-90b0-29b10cb195b0",
   "metadata": {},
   "source": [
    "### 1. Indexing:\n",
    "- **Loading documents**:\n",
    "  - We need to first load the articles we already have.\n",
    "  - We can use [DocumentLoaders](https://python.langchain.com/docs/concepts/document_loaders/) for this, which are objects\n",
    "    that load in data from a source and return a list of Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e789774-9742-40a9-be25-1b9da336fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 15/15 [00:00<00:00, 545.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 15\n",
      "Total characters in the first document: 30930\n",
      "First 200 characters of the first document:\n",
      " Willkommen zum zweiten Teil für konzentrierte Systeme zur Nutzung der Solarenergie. Ich wollte mit Ihnen über Solarturme sprechen, aber vorher noch über ein paar Besonderheiten bezüglich der Parabolr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Step 1: Load all .txt files from a aricles directory\n",
    "loader = DirectoryLoader(\n",
    "    path=\"articles\",  # Path to the directory\n",
    "    glob=\"**/*.txt\",                # Pattern to match .txt files\n",
    "    loader_cls=TextLoader,          # Use TextLoader for .txt files\n",
    "    show_progress=True ,           # Show progress bar while loading\n",
    ")\n",
    "\n",
    "# Step 2: Load the documents\n",
    "docs = loader.load()\n",
    "\n",
    "# Step 3: Verify the loaded documents\n",
    "print(f\"Number of documents loaded: {len(docs)}\")\n",
    "print(f\"Total characters in the first document: {len(docs[0].page_content)}\")\n",
    "print(\"First 200 characters of the first document:\")\n",
    "print(docs[0].page_content[:200])  # Print the first 200 characters for a quick preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f5bd1-672d-4112-9af5-27f007203f9e",
   "metadata": {},
   "source": [
    "### 2. Splitting Documents\n",
    "Splitting documents into smaller chunks ensures they fit within a model's context window and improves retrieval precision. It also enhances embedding quality and allows for parallel processing, making the system more efficient.\n",
    "\n",
    "We will use a [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84cee01a-e6d3-49bc-905f-905977dfb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 1006 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# Import the RecursiveCharacterTextSplitter class\n",
    "# This is a text splitter that recursively splits documents into smaller chunks\n",
    "# using common separators like new lines, spaces, and punctuation.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter with the following parameters:\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # The maximum size (in characters) of each chunk.\n",
    "                      # Documents will be split into chunks of this size or smaller.\n",
    "    \n",
    "    chunk_overlap=200,  # The number of characters that adjacent chunks will overlap.\n",
    "                        # Overlapping helps preserve context between chunks.\n",
    "    \n",
    "    add_start_index=True,  # If True, adds a metadata field `start_index` to each chunk,\n",
    "                           # which tracks the starting position of the chunk in the original document.\n",
    ")\n",
    "\n",
    "# Split the documents into smaller chunks using the text splitter\n",
    "# `docs` is a list of Document objects (e.g., loaded from a file or directory).\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb18c6e-d9b3-433d-bf69-09bfe3d6f825",
   "metadata": {},
   "source": [
    "### 3. Storing documents\n",
    "\n",
    "**What is Embedding?**\n",
    "**Embedding** is the process of converting text into numerical vectors (arrays of numbers) that capture the semantic meaning of the text. These vectors allow us to perform mathematical operations, such as calculating similarity between texts.\n",
    "\n",
    "Now we need to index our generated text chunks so that we can search over them at runtime. \n",
    "our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97687a1-7a8d-401f-a383-cee034e90be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in vector store: 1006\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required modules\n",
    "import os  # For interacting with the operating system (e.g., environment variables)\n",
    "from langchain_mistralai import MistralAIEmbeddings  # For MistralAI embeddings\n",
    "from langchain_community.vectorstores import FAISS  # For FAISS vector storage\n",
    "from dotenv import load_dotenv  # For loading environment variables from a .env file\n",
    "\n",
    "# Step 2: Load environment variables from a .env file\n",
    "# This is useful for storing sensitive information like API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Step 3: Set up MistralAI API key\n",
    "# Check if the MistralAI API key is already set in the environment variables\n",
    "mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"MISTRAL_API_KEY not found in .env file. Please add it.\")\n",
    "\n",
    "# Step 4: Set up Hugging Face Token (optional, if needed for other tasks)\n",
    "# Check if the Hugging Face token is already set in the environment variables\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    # If not, try to get it from another environment variable (e.g., HF_API_TOKEN)\n",
    "    hf_token = os.environ.get(\"HF_API_TOKEN\")\n",
    "    if hf_token:\n",
    "        os.environ[\"HF_TOKEN\"] = hf_token  # Set HF_TOKEN if HF_API_TOKEN is found\n",
    "    else:\n",
    "        print(\"Warning: Hugging Face token not found in .env file. Some features may not work.\")\n",
    "\n",
    "# Step 5: Initialize MistralAI embeddings\n",
    "# Use the MistralAI embedding model (\"mistral-embed\") to convert text into numerical vectors\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=mistral_api_key)\n",
    "\n",
    "# Step 6: Create a FAISS vector store\n",
    "# FAISS is a library for efficient similarity search and clustering of dense vectors\n",
    "# `from_documents` creates a vector store from the split documents (`all_splits`) and their embeddings\n",
    "vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)\n",
    "\n",
    "# Step 7: Verify the documents were added to the vector store\n",
    "# FAISS does not return document IDs, but you can check the number of documents stored in the vector store\n",
    "# `vector_store.docstore._dict` contains the documents, and its length gives the number of documents\n",
    "print(f\"Number of documents in vector store: {len(vector_store.docstore._dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075a179-9267-41cc-a210-c404ab8898d4",
   "metadata": {},
   "source": [
    "### 4. Retrieval and Generation\n",
    "\n",
    "We’ll use **LangGraph** to tie together the **retrieval** and **generation** steps into a single application. This makes our RAG pipeline more modular, scalable, and easier to debug.\n",
    "\n",
    "#### What is LangGraph?\n",
    "\n",
    "LangGraph is a tool that helps you build workflows for your application. Think of it like a recipe where each step is clearly defined, and you can easily see how the steps connect to each other.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Use LangGraph for RAG?\n",
    "\n",
    "Imagine you’re building a RAG application that does two main things:\n",
    "1. **Retrieval**: Find relevant information from a knowledge base (like searching a library).\n",
    "2. **Generation**: Use a language model (like MistralAI) to generate an answer based on the retrieved information.\n",
    "\n",
    "LangGraph helps you organize these steps and make them work together smoothly.\n",
    "\n",
    "---\n",
    "\n",
    "#### How LangGraph Helps\n",
    "\n",
    "1. **Modular Design**:\n",
    "   - Break your RAG pipeline into smaller steps (e.g., retrieval, generation).\n",
    "   - Each step is like a building block that you can test and reuse.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Handle multiple requests at the same time (e.g., answering many questions at once).\n",
    "   - Support streaming (real-time answers), async (non-blocking), and batched (processing multiple inputs together).\n",
    "\n",
    "3. **Debugging**:\n",
    "   - Use LangSmith (a debugging tool) to see what happens at each step of your pipeline.\n",
    "   - This helps you find and fix problems easily.\n",
    "\n",
    "4. **Flexibility**:\n",
    "   - Add new features (like saving results or asking for human approval) without rewriting your entire code.\n",
    "\n",
    "---\n",
    "\n",
    "#### Simple Example\n",
    "\n",
    "Imagine you’re building a RAG application to answer questions. Here’s how LangGraph helps:\n",
    "\n",
    "1. **Step 1: Retrieve Documents**:\n",
    "   - You ask a question (e.g., \"What is LangChain?\").\n",
    "   - The system searches a knowledge base (like a library) to find relevant information.\n",
    "\n",
    "2. **Step 2: Generate an Answer**:\n",
    "   - The system takes the retrieved information and uses a language model (like MistralAI) to generate an answer.\n",
    "\n",
    "3. **LangGraph Workflow**:\n",
    "   - LangGraph connects these two steps into a workflow.\n",
    "   - You can easily add more steps (e.g., saving the answer or asking for human approval).\n",
    "\n",
    "---\n",
    "\n",
    "### Why is This Useful?\n",
    "\n",
    "- **Reusability**: You can reuse the same workflow for different tasks (e.g., answering questions, summarizing documents).\n",
    "- **Scalability**: Handle many questions at once without slowing down.\n",
    "- **Debugging**: Easily find and fix problems in your pipeline.\n",
    "- **Flexibility**: Add new features without rewriting everything.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "LangGraph makes it easy to build and manage complex workflows (like RAG) by breaking them into smaller, reusable steps. It’s like having a recipe for your application that you can tweak and improve over time.\n",
    "\n",
    "\n",
    "To use **LangGraph**, we need to define three things:\n",
    "\n",
    "1. The state of our application: controls what data is input to the application, transferred between steps, and output by the application.\n",
    "2. The nodes of our application (i.e., application steps);\n",
    "3. The \"control flow\" of our application (e.g., the ordering of the steps): compiling the steps into a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b354a07-096c-47d5-8fe0-86004f277cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required modules\n",
    "from langchain_core.documents import Document  # For handling document objects\n",
    "from typing_extensions import List, TypedDict  # For type hints\n",
    "from langgraph.graph import START, StateGraph  # For building the workflow graph\n",
    "from langchain_core.prompts import PromptTemplate  # For customizing the prompt\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Step 2: Set up the Mistral API key\n",
    "# If the Mistral API key is not already set in the environment, prompt the user to enter it\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter API key for Mistral AI: \")\n",
    "\n",
    "# Step 3: Initialize the Mistral language model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the Mistral language model with the specified model name and provider\n",
    "llm = init_chat_model(\"mistral-large-latest\", model_provider=\"mistralai\")\n",
    "\n",
    "# Step 4: Define the State\n",
    "# The State is a dictionary that holds the data passed between steps in the workflow\n",
    "class State(TypedDict):\n",
    "    question: str  # The user's question\n",
    "    context: List[Document]  # The retrieved documents\n",
    "    answer: str  # The generated answer\n",
    "\n",
    "# Step 5: Define the retrieval step\n",
    "def retrieve(state: State):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents for the given question.\n",
    "    \n",
    "    Args:\n",
    "        state (State): The current state of the workflow, containing the question.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the \"context\" key containing the retrieved documents.\n",
    "    \"\"\"\n",
    "    # Search the vector store for documents relevant to the question\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}  # Update the state with the retrieved documents\n",
    "\n",
    "# Step 6: Define the enhanced prompt template\n",
    "template = \"\"\"\n",
    "You are a knowledgeable and helpful assistant. Your task is to answer the user's question based on the provided context. Follow these rules strictly:\n",
    "\n",
    "1. **Language**:\n",
    "   - The answer must be written exclusively in **German**.\n",
    "\n",
    "2. **Document Processing**:\n",
    "   - Carefully analyze all provided documents to extract relevant information.\n",
    "   - Do not provide an answer until you have checked all documents.\n",
    "\n",
    "3. **Answer Synthesis**:\n",
    "   - Combine all relevant information from the documents into a single, well-structured answer.\n",
    "   - The answer must be exclusively from the documents only. Do not generate answers from your own knowledge.\n",
    "   - For each piece of information, you must follow this style:\n",
    "       [Dokumentname: [Dokumentname] | Absatz: [Absatzindex] | Zeilen: [Startzeile - Endzeile]] : [die Antwort]\n",
    "\n",
    "4. **Answer Structure**:\n",
    "   - Begin your answer with a brief summary of the key points.\n",
    "   - List every single piece of relevant information found in the documents, formatted as specified.\n",
    "   - Ensure the answer is concise, clear, and easy to read.\n",
    "   - Avoid redundancy and focus on providing unique information.\n",
    "\n",
    "5. **Source Attribution**:\n",
    "   - Always include the document name, paragraph index, and line numbers for each piece of information.\n",
    "   - If the document name is missing, use \"Unbekannt\" as the document name.\n",
    "   - If paragraph index or line numbers are missing, use \"N/A\" as a placeholder.\n",
    "\n",
    "6. **Handling Missing Information**:\n",
    "   - If the context does not contain enough information to answer the question, say \"Ich weiß es nicht.\"\n",
    "   - Do not make up or assume any information.\n",
    "\n",
    "---\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Antwort:\n",
    "\"\"\"\n",
    "\n",
    "# Step 7: Define the generation step\n",
    "def generate(state: State):\n",
    "    \"\"\"\n",
    "    Generates an answer using the retrieved documents and the question.\n",
    "    \n",
    "    Args:\n",
    "        state (State): The current state of the workflow, containing the question and context.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the \"answer\" key containing the generated response.\n",
    "    \"\"\"\n",
    "    # Combine the content of all retrieved documents into a single string with source information\n",
    "    docs_content = \"\\n\\n\".join(\n",
    "        f\"Dokumentname: {doc.metadata.get('source', 'Unbekannt')}\\n\"\n",
    "        f\"Dokument-ID: {doc.id}\\n\"\n",
    "        f\"Inhalt: {doc.page_content}\"\n",
    "        for doc in state[\"context\"]\n",
    "    )\n",
    "    \n",
    "    # Define the enhanced prompt template\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    # Format the prompt using the PromptTemplate\n",
    "    formatted_prompt = prompt_template.format(question=state[\"question\"], context=docs_content)\n",
    "    \n",
    "    # Generate a response using the language model\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Step 8: Build the workflow graph\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add the retrieval and generation steps to the graph\n",
    "graph_builder.add_sequence([retrieve, generate])\n",
    "\n",
    "# Define the starting point of the workflow\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "\n",
    "# Compile the graph into an executable workflow\n",
    "graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b712db5-09ab-4cc4-a980-ddfe6006b10d",
   "metadata": {},
   "source": [
    "# 3. Creating the API endpoint to communicate with the bot\n",
    "\n",
    "Creating an API endpoint is essential for enabling communication between the frontend (e.g., a web or mobile app) and the backend (our RAG-based bot). This endpoint acts as a bridge, allowing users to send queries (e.g., text inputs) to the bot and receive responses in real-time. By encapsulating the bot's logic in an API, we ensure modularity, scalability, and ease of integration with various client applications. Additionally, it centralizes the processing of requests, making it easier to manage, debug, and extend the system in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc312dfc-27fe-4dc9-bdff-4034792ab2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask app is running on http://127.0.0.1:5800/\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5800\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [23/Mar/2025 09:37:35] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:37:39] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:40:45] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:40:57] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:42:43] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:43:08] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:43:44] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:43:56] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:44:39] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:44:40] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:44:55] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:45:01] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:45:24] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:45:39] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:46:38] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:46:54] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:47:25] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:47:26] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:48:01] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:48:24] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:49:18] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:49:36] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:50:41] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:51:09] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:51:43] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:52:05] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:52:48] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:53:19] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:54:12] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:54:33] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:55:05] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:55:17] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:55:43] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:55:44] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:55:50] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:55:51] \"\u001b[31m\u001b[1mPOST /api HTTP/1.1\u001b[0m\" 400 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:56:01] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:56:15] \"POST /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:56:49] \"OPTIONS /api HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Mar/2025 09:56:58] \"POST /api HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "# Create an instance of the Flask class\n",
    "app = Flask(__name__)\n",
    "\n",
    "CORS(app)\n",
    "\n",
    "port = 5800\n",
    "\n",
    "@app.route(\"/api\", methods=['POST'])\n",
    "def chat():\n",
    "    errors = []\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            # Get JSON data from the request\n",
    "            data = request.json\n",
    "            question = data['message']\n",
    "            answer = graph.invoke({\"question\": question})['answer']\n",
    "            # Process the data (example: echo the data back)\n",
    "            response = {\n",
    "                \"data\": answer\n",
    "            }\n",
    "            return jsonify(response), 200\n",
    "        except Exception as e:\n",
    "            errors.append(str(e))  # Add the error to the errors list\n",
    "    else:\n",
    "        errors.append(\"Invalid request method\")  # Add error for invalid method\n",
    "\n",
    "    # If there are errors, return them in the response\n",
    "    if errors:\n",
    "        return jsonify({\"errors\": errors}), 400\n",
    "\n",
    "def run_flask():\n",
    "    print(f\"Flask app is running on http://127.0.0.1:{port}/\")\n",
    "    app.run(port=port, debug=False, use_reloader=False)\n",
    "\n",
    "# Global variable to track the Flask thread\n",
    "flask_thread = None\n",
    "\n",
    "# Main entry point\n",
    "if __name__ == '__main__':\n",
    "    # Start the Flask app in a separate thread\n",
    "    flask_thread = threading.Thread(target=run_flask)\n",
    "    flask_thread.daemon = True  # Daemonize the thread so it exits when the main program exits\n",
    "    flask_thread.start()\n",
    "\n",
    "    try:\n",
    "        # Keep the main thread alive to keep the Flask app running\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Shutting down Flask app...\")\n",
    "        # No need to explicitly stop the thread since it's daemonized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
